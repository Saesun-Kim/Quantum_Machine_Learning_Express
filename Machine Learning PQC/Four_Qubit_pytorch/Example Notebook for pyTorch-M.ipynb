{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852aa1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c263dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "# Load in some additional functions from a text file\n",
    "from helpers import parity\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Pyfiles')\n",
    "from circuits_n1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ba8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the QuantumInstance we'll use to run things\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795e0fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd6fa3",
   "metadata": {},
   "source": [
    "# Pull in a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183b6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this\n",
    "dataSetID = '0'\n",
    "\n",
    "# Define the directory paths\n",
    "data0Path = r'../../dataset/data{0}.txt'.format(dataSetID)\n",
    "data0Label = r'../../dataset/data{0}label.txt'.format(dataSetID)\n",
    "\n",
    "# Load in the data\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbb6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_validate_test_data(data, train_size=100, validate_size=500, randomSeed=0):\n",
    "    r'''This is a function which, given a dataset, will return 3 distinct datasets from it: a training dataset,\n",
    "    a validation dataset, and a testing dataset.\n",
    "    \n",
    "    The size of the training and validation datasets is set by the function call; the size of the testing\n",
    "    dataset is the remainder of the data after training & validation dataset have been generated.'''\n",
    "    \n",
    "    assert len(data) > train_size+validate_size, 'Not enough data to do the splitting.'\n",
    "    \n",
    "    def generate_data(data, ixs):\n",
    "        r'''Helper function for generating data.'''\n",
    "        X= [np.array(list(flatten([data[j][0],data[j][0]]))) for j in ixs]\n",
    "        y = [data[j][1] for j in ixs]    \n",
    "    \n",
    "        # Recast X as a pyTorch tensor\n",
    "        X = Tensor(X)\n",
    "        \n",
    "        # Change how the data is labeled: {-1,+1} --> {0, 1}\n",
    "        y =  [ (x + 1)/2 for x in y]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    # At the start, we could use all possible datapoints for training\n",
    "    possible_ixs = range(len(data))\n",
    "    \n",
    "    # Training data\n",
    "    np.random.seed(randomSeed)\n",
    "    train_ixs = np.random.choice(possible_ixs, size=train_size)\n",
    "    train_X, train_y = generate_data(data, train_ixs)\n",
    "    \n",
    "    # Update the possible indices we could choose from\n",
    "    possible_ixs = [x for x in possible_ixs if x not in train_ixs]\n",
    "\n",
    "    # Validation data\n",
    "    np.random.seed(randomSeed)\n",
    "    validate_ixs = np.random.choice(possible_ixs, size=validate_size)\n",
    "    validate_X, validate_y = generate_data(data, validate_ixs)\n",
    "    \n",
    "    # Now, use the rest of the data for testing\n",
    "    possible_ixs = [x for x in possible_ixs if x not in validate_ixs]\n",
    "    test_X, test_y = generate_data(data, possible_ixs)\n",
    "    \n",
    "    return train_X, train_y, validate_X, validate_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c981eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_X, train_y, validate_X, validate_y, test_X, test_y = generate_train_validate_test_data(data, train_size=100, validate_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbc994",
   "metadata": {},
   "source": [
    "# Make the circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51046038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌─────────────────────────────┐┌───────────────────────────────┐\n",
       "q_0: ┤0                            ├┤0                              ├\n",
       "     │                             ││                               │\n",
       "q_1: ┤1                            ├┤1                              ├\n",
       "     │  Embed(x[0],x[1],x[2],x[3]) ││  PQC(θ0,θ1,θ2,θ3,θ4,θ5,θ6,θ7) │\n",
       "q_2: ┤2                            ├┤2                              ├\n",
       "     │                             ││                               │\n",
       "q_3: ┤3                            ├┤3                              ├\n",
       "     └─────────────────────────────┘└───────────────────────────────┘</pre>"
      ],
      "text/plain": [
       "     ┌─────────────────────────────┐┌───────────────────────────────┐\n",
       "q_0: ┤0                            ├┤0                              ├\n",
       "     │                             ││                               │\n",
       "q_1: ┤1                            ├┤1                              ├\n",
       "     │  Embed(x[0],x[1],x[2],x[3]) ││  PQC(θ0,θ1,θ2,θ3,θ4,θ5,θ6,θ7) │\n",
       "q_2: ┤2                            ├┤2                              ├\n",
       "     │                             ││                               │\n",
       "q_3: ┤3                            ├┤3                              ├\n",
       "     └─────────────────────────────┘└───────────────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the feature map\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "\n",
    "for j in [0, 2]:\n",
    "    feature_map.ry(np.pi/4,j)\n",
    "    feature_map.ry(np.pi/4,j+1)\n",
    "    feature_map.rz(np.pi/4,j)\n",
    "    feature_map.rz(np.pi/4,j+1)\n",
    "\n",
    "# Make an ansazt\n",
    "param_y=[(Parameter('θ'+str(i))) for i in range(12)]\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "ansatz=circuit1(ansatz,param_y,1,0)\n",
    "\n",
    "\n",
    "qc = QuantumCircuit(ansatz.width())\n",
    "qc.append(feature_map, range(ansatz.width()))\n",
    "qc.append(ansatz, range(ansatz.width()))\n",
    "\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29f8c7",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e76a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, X, y_target):\n",
    "    r'''Helper function to compute the accuracy'''\n",
    "    \n",
    "    # Evaluate model on input data\n",
    "    output = model(X)\n",
    "    \n",
    "    # Now, do some wrangling to get the data in a better format\n",
    "    output = output.detach().numpy()\n",
    "    \n",
    "    # Output is a list of lists, where the inner list is the probabilities\n",
    "    # of each class assignment. We pick the most probable class as the prediction\n",
    "    predictions = np.array([np.argmax(x) for x in output])\n",
    "    \n",
    "    return sum(predictions == y_target)/len(y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8487ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .5\n",
    "epochs = 20\n",
    "output_shape = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  28.395334243774414\n",
      "__Loss is  28.35608673095703\n",
      "__Loss is  28.31281089782715\n",
      "__Loss is  28.26523780822754\n",
      "__Loss is  28.213159561157227\n",
      "__Loss is  28.156373977661133\n",
      "__Loss is  28.09478187561035\n",
      "__Loss is  28.02828598022461\n",
      "__Loss is  27.956897735595703\n",
      "__Loss is  27.880706787109375\n",
      "__Loss is  27.799880981445312\n",
      "__Loss is  27.714706420898438\n",
      "__Loss is  27.62554359436035\n",
      "__Loss is  27.532869338989258\n",
      "__Loss is  27.43724250793457\n",
      "__Loss is  27.339303970336914\n",
      "__Loss is  27.239728927612305\n",
      "__Loss is  27.139244079589844\n",
      "__Loss is  27.03860855102539\n",
      "Accuracies are 0.49 0.472 0.472\n",
      "Learning Rate is  0.04\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  28.27850341796875\n",
      "__Loss is  28.06242561340332\n",
      "__Loss is  27.773792266845703\n",
      "__Loss is  27.418380737304688\n",
      "__Loss is  27.02275848388672\n",
      "__Loss is  26.629058837890625\n",
      "__Loss is  26.277498245239258\n",
      "__Loss is  25.99020767211914\n",
      "__Loss is  25.769092559814453\n",
      "__Loss is  25.60418701171875\n",
      "__Loss is  25.482385635375977\n",
      "__Loss is  25.392030715942383\n",
      "__Loss is  25.324209213256836\n",
      "__Loss is  25.27248764038086\n",
      "__Loss is  25.232421875\n",
      "__Loss is  25.20087432861328\n",
      "__Loss is  25.175643920898438\n",
      "__Loss is  25.155193328857422\n",
      "__Loss is  25.138423919677734\n",
      "Accuracies are 0.46 0.452 0.452\n",
      "Learning Rate is  0.09\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  28.052663803100586\n",
      "__Loss is  27.36155128479004\n",
      "__Loss is  26.481952667236328\n",
      "__Loss is  25.80290985107422\n",
      "__Loss is  25.449874877929688\n",
      "__Loss is  25.281991958618164\n",
      "__Loss is  25.195039749145508\n",
      "__Loss is  25.144960403442383\n",
      "__Loss is  25.113548278808594\n",
      "__Loss is  25.09256362915039\n",
      "__Loss is  25.07785987854004\n",
      "__Loss is  25.067180633544922\n",
      "__Loss is  25.05922508239746\n",
      "__Loss is  25.05317497253418\n",
      "__Loss is  25.048490524291992\n",
      "__Loss is  25.044809341430664\n",
      "__Loss is  25.041889190673828\n",
      "__Loss is  25.039552688598633\n",
      "__Loss is  25.03766632080078\n",
      "Accuracies are 0.37 0.446 0.446\n",
      "Learning Rate is  0.16\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  27.685821533203125\n",
      "__Loss is  26.227476119995117\n",
      "__Loss is  25.36143684387207\n",
      "__Loss is  25.167720794677734\n",
      "__Loss is  25.10427474975586\n",
      "__Loss is  25.074785232543945\n",
      "__Loss is  25.05852699279785\n",
      "__Loss is  25.048686981201172\n",
      "__Loss is  25.042383193969727\n",
      "__Loss is  25.03818702697754\n",
      "__Loss is  25.03533935546875\n",
      "__Loss is  25.03335189819336\n",
      "__Loss is  25.03194808959961\n",
      "__Loss is  25.030954360961914\n",
      "__Loss is  25.03021240234375\n",
      "__Loss is  25.02967643737793\n",
      "__Loss is  25.029277801513672\n",
      "__Loss is  25.02897834777832\n",
      "__Loss is  25.028743743896484\n",
      "Accuracies are 0.46 0.472 0.472\n",
      "Learning Rate is  0.25\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  27.165786743164062\n",
      "__Loss is  25.3055477142334\n",
      "__Loss is  25.100669860839844\n",
      "__Loss is  25.06183433532715\n",
      "__Loss is  25.045955657958984\n",
      "__Loss is  25.0380802154541\n",
      "__Loss is  25.033855438232422\n",
      "__Loss is  25.031471252441406\n",
      "__Loss is  25.03009796142578\n",
      "__Loss is  25.029277801513672\n",
      "__Loss is  25.028776168823242\n",
      "__Loss is  25.028472900390625\n",
      "__Loss is  25.028284072875977\n",
      "__Loss is  25.02815818786621\n",
      "__Loss is  25.028074264526367\n",
      "__Loss is  25.02801513671875\n",
      "__Loss is  25.027973175048828\n",
      "__Loss is  25.027950286865234\n",
      "__Loss is  25.027931213378906\n",
      "Accuracies are 0.45 0.49 0.49\n",
      "Learning Rate is  0.36\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  26.531505584716797\n",
      "__Loss is  25.047107696533203\n",
      "__Loss is  25.036684036254883\n",
      "__Loss is  25.031940460205078\n",
      "__Loss is  25.02974510192871\n",
      "__Loss is  25.028745651245117\n",
      "__Loss is  25.02827262878418\n",
      "__Loss is  25.028060913085938\n",
      "__Loss is  25.027956008911133\n",
      "__Loss is  25.027915954589844\n",
      "__Loss is  25.027891159057617\n",
      "__Loss is  25.027875900268555\n",
      "__Loss is  25.027875900268555\n",
      "__Loss is  25.027862548828125\n",
      "__Loss is  25.02787208557129\n",
      "__Loss is  25.027875900268555\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.02786636352539\n",
      "__Loss is  25.027873992919922\n",
      "Accuracies are 0.45 0.49 0.49\n",
      "Learning Rate is  0.49\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  25.890756607055664\n",
      "__Loss is  25.034719467163086\n",
      "__Loss is  25.032052993774414\n",
      "__Loss is  25.03045082092285\n",
      "__Loss is  25.029460906982422\n",
      "__Loss is  25.028860092163086\n",
      "__Loss is  25.028478622436523\n",
      "__Loss is  25.02824592590332\n",
      "__Loss is  25.028104782104492\n",
      "__Loss is  25.02800941467285\n",
      "__Loss is  25.027957916259766\n",
      "__Loss is  25.027931213378906\n",
      "__Loss is  25.027904510498047\n",
      "__Loss is  25.027889251708984\n",
      "__Loss is  25.027883529663086\n",
      "__Loss is  25.027883529663086\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.027875900268555\n",
      "__Loss is  25.027877807617188\n",
      "Accuracies are 0.45 0.49 0.49\n",
      "Learning Rate is  0.64\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  25.389373779296875\n",
      "__Loss is  25.034027099609375\n",
      "__Loss is  25.030776977539062\n",
      "__Loss is  25.029354095458984\n",
      "__Loss is  25.028629302978516\n",
      "__Loss is  25.02826690673828\n",
      "__Loss is  25.028072357177734\n",
      "__Loss is  25.027973175048828\n",
      "__Loss is  25.027931213378906\n",
      "__Loss is  25.02789878845215\n",
      "__Loss is  25.027881622314453\n",
      "__Loss is  25.027881622314453\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.02787208557129\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.027877807617188\n",
      "__Loss is  25.027870178222656\n",
      "Accuracies are 0.45 0.49 0.49\n",
      "Learning Rate is  0.81\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  25.118621826171875\n",
      "__Loss is  25.029077529907227\n",
      "__Loss is  25.028093338012695\n",
      "__Loss is  25.027938842773438\n",
      "__Loss is  25.02789878845215\n",
      "__Loss is  25.02788543701172\n",
      "__Loss is  25.027873992919922\n",
      "__Loss is  25.027877807617188\n",
      "__Loss is  25.02786636352539\n",
      "__Loss is  25.027877807617188\n",
      "__Loss is  25.027875900268555\n",
      "__Loss is  25.02787208557129\n",
      "__Loss is  25.02786636352539\n",
      "__Loss is  25.027864456176758\n",
      "__Loss is  25.02786636352539\n",
      "__Loss is  25.02786636352539\n",
      "__Loss is  25.027868270874023\n",
      "__Loss is  25.027870178222656\n",
      "__Loss is  25.027870178222656\n",
      "Accuracies are 0.45 0.49 0.49\n",
      "Learning Rate is  1.0\n",
      "__Loss is  28.43077850341797\n",
      "__Loss is  25.037729263305664\n",
      "__Loss is  25.028575897216797\n",
      "__Loss is  25.028059005737305\n",
      "__Loss is  25.02793312072754\n",
      "__Loss is  25.02788734436035\n",
      "__Loss is  25.027875900268555\n"
     ]
    }
   ],
   "source": [
    "learning_rate=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "storage_model=[]\n",
    "for l in learning_rate: \n",
    "    # Create the QNN:\n",
    "    qnn = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "\n",
    "    # Set up the pyTorch model\n",
    "    np.random.seed(0)  \n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn.num_weights) - 1)\n",
    "    model = TorchConnector(qnn, initial_weights)\n",
    "\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.SGD(model.parameters(),lr=l)\n",
    "    f_loss = MSELoss(reduction='mean')\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()   \n",
    "\n",
    "    print(\"Learning Rate is \", l)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(train_X, train_y):                        # evaluate batch loss\n",
    "            output = model(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    # save accuracy and parameters\n",
    "    output_train_X = model(train_X)\n",
    "    output_validate_X = model(validate_X)\n",
    "    output_test_X = model(test_X)\n",
    "    #Loading is taking most of the time.^^\n",
    "    predictions_train_X = np.array([np.argmax(x) for x in output_train_X.detach().numpy()])    \n",
    "    predictions_validate_X = np.array([np.argmax(x) for x in output_validate_X.detach().numpy()])\n",
    "    predictions_test_X = np.array([np.argmax(x) for x in output_test_X.detach().numpy()])\n",
    "    accuracy_train = np.round(sum(predictions_train_X == train_y)/len(train_y), 4)\n",
    "    accuracy_validate = np.round(sum(predictions_validate_X == validate_y)/len(validate_y), 4)\n",
    "    accuracy_test = np.round(sum(predictions_test_X == test_y)/len(test_y), 4)\n",
    "    print(\"Accuracies are\",accuracy_train,accuracy_validate,accuracy_validate)\n",
    "    param_model=[p.data for p in model.parameters()]\n",
    "    storage_model.append([l,param_model,accuracy_train,accuracy_validate,accuracy_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f233607",
   "metadata": {},
   "source": [
    "# Evaluate the accuracy of the trained network\n",
    "\n",
    "We'll make plots of the different data sets, and circle which data points the trained model gets wrong.\n",
    "We'll also title each plot with the value of the accuracy of the model on that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964ccb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_accuracy_plot(X_data, y_target, model, ax):\n",
    "    output = model(X_data)\n",
    "    predictions = np.array([np.argmax(x) for x in output.detach().numpy()])\n",
    "    for x, y_tar, y_pred in zip(X_data, y_target, predictions):\n",
    "        if y_tar == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_tar != y_pred:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='y', linewidths=2)\n",
    "\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model(Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, levels=30, zorder=-1, cmap='bwr')\n",
    "\n",
    "    accuracy = np.round(sum(predictions == y_target)/len(y_target), 4)\n",
    "\n",
    "    return ax, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b01166",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "ax1, accuracy1 = make_accuracy_plot(train_X, train_y, model, ax1)\n",
    "ax2, accuracy2 = make_accuracy_plot(validate_X, validate_y, model, ax2)\n",
    "ax3, accuracy3 = make_accuracy_plot(test_X, test_y, model, ax3)\n",
    "\n",
    "ax1.set_title('Train: {0}'.format(accuracy1))\n",
    "ax2.set_title('Validate: {0}'.format(accuracy2))\n",
    "ax3.set_title('Test: {0}'.format(accuracy3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98e6f9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e632387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "Accuracies are 0.49 0.472 0.472\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "output_train_X = model(train_X)\n",
    "print(1)\n",
    "output_validate_X = model(validate_X)\n",
    "print(1)\n",
    "output_test_X = model(test_X)\n",
    "print(1)\n",
    "predictions_train_X = np.array([np.argmax(x) for x in output_train_X.detach().numpy()])    \n",
    "print(2)\n",
    "predictions_validate_X = np.array([np.argmax(x) for x in output_validate_X.detach().numpy()])\n",
    "print(2)\n",
    "predictions_test_X = np.array([np.argmax(x) for x in output_test_X.detach().numpy()])\n",
    "print(2)\n",
    "accuracy_train = np.round(sum(predictions_train_X == train_y)/len(train_y), 4)\n",
    "print(3)\n",
    "accuracy_validate = np.round(sum(predictions_validate_X == validate_y)/len(validate_y), 4)\n",
    "print(3)\n",
    "accuracy_test = np.round(sum(predictions_test_X == test_y)/len(test_y), 4)\n",
    "print(3)\n",
    "print(\"Accuracies are\",accuracy_train,accuracy_validate,accuracy_validate)\n",
    "param_model=[p.data for p in model.parameters()]\n",
    "print(4)\n",
    "storage_model.append([l,param_model,accuracy_train,accuracy_validate,accuracy_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75655818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
