{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "data0Path = r'../../../dataset/data2c.txt'\n",
    "data0Label = r'../../../dataset/data2clabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data1 = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return ('0'*(4-len('{:b}'.format(x) ))+'{:b}'.format(x))\n",
    "def firsttwo(x):\n",
    "    return x[:2]\n",
    "parity = lambda x: firsttwo(binary(x)).count('1') % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 206.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "#data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "data_ixs = np.random.choice(len(data1), size=100)\n",
    "\n",
    "X= [np.array(list(flatten([data1[j][0],data1[j][0]]))) for j in data_ixs]\n",
    "y = [data1[j][1] for j in data_ixs]\n",
    "y01 =  [ (x + 1)/2 for x in y]\n",
    "X_ = Tensor(X)\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "\n",
    "num_inputs=4;\n",
    "\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "feature_map.ry(pi/4,0)\n",
    "feature_map.ry(pi/4,1)\n",
    "feature_map.ry(pi/4,2)\n",
    "feature_map.ry(pi/4,3)\n",
    "feature_map.rz(pi/4,0)\n",
    "feature_map.rz(pi/4,1)\n",
    "feature_map.rz(pi/4,2)\n",
    "feature_map.rz(pi/4,3)\n",
    "\n",
    "\n",
    "param_y=[];\n",
    "for i in range(12):\n",
    "    param_y.append((Parameter('θ'+str(i))))\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "for i in range(4):\n",
    "    ansatz.ry(param_y[i],i)\n",
    "for i in range(4):\n",
    "    ansatz.rz(param_y[i+4],i)\n",
    "\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.append(feature_map, range(num_inputs))\n",
    "qc.append(ansatz, range(num_inputs))\n",
    "\n",
    "ansatz.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saesun Kim\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Loss is  56.78873062133789\n",
      "__Loss is  56.467498779296875\n",
      "__Loss is  56.1558837890625\n",
      "__Loss is  55.85404586791992\n",
      "__Loss is  55.56214904785156\n",
      "__Loss is  55.28028106689453\n",
      "__Loss is  55.00850296020508\n",
      "__Loss is  54.74684143066406\n",
      "__Loss is  54.49536895751953\n",
      "__Loss is  54.253997802734375\n",
      "Learning Rate is  0.04\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  55.5599365234375\n",
      "__Loss is  54.48527526855469\n",
      "__Loss is  53.563045501708984\n",
      "__Loss is  52.7872200012207\n",
      "__Loss is  52.14780807495117\n",
      "__Loss is  51.63161087036133\n",
      "__Loss is  51.22336196899414\n",
      "__Loss is  50.90679168701172\n",
      "__Loss is  50.66582489013672\n",
      "Learning Rate is  0.09\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  54.233272552490234\n",
      "__Loss is  52.43232727050781\n",
      "__Loss is  51.2827033996582\n",
      "__Loss is  50.622154235839844\n",
      "__Loss is  50.28034210205078\n",
      "__Loss is  50.1201057434082\n",
      "__Loss is  50.05145263671875\n",
      "__Loss is  50.024410247802734\n",
      "__Loss is  50.01509094238281\n",
      "Learning Rate is  0.16\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  52.75224685668945\n",
      "__Loss is  50.84256362915039\n",
      "__Loss is  50.193904876708984\n",
      "__Loss is  50.038333892822266\n",
      "__Loss is  50.01386260986328\n",
      "__Loss is  50.02449417114258\n",
      "__Loss is  50.072547912597656\n",
      "__Loss is  50.1756477355957\n",
      "__Loss is  50.33076477050781\n",
      "Learning Rate is  0.25\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  51.43020248413086\n",
      "__Loss is  50.14488983154297\n",
      "__Loss is  50.01479721069336\n",
      "__Loss is  50.04640197753906\n",
      "__Loss is  50.24851608276367\n",
      "__Loss is  50.68339920043945\n",
      "__Loss is  51.13393020629883\n",
      "__Loss is  51.277957916259766\n",
      "__Loss is  51.060794830322266\n",
      "Learning Rate is  0.36\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.52589797973633\n",
      "__Loss is  50.01541519165039\n",
      "__Loss is  50.12102508544922\n",
      "__Loss is  50.831016540527344\n",
      "__Loss is  51.81846237182617\n",
      "__Loss is  51.884742736816406\n",
      "__Loss is  51.15707015991211\n",
      "__Loss is  50.46152877807617\n",
      "__Loss is  50.12599182128906\n",
      "Learning Rate is  0.49\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.11022186279297\n",
      "__Loss is  50.084068298339844\n",
      "__Loss is  51.406028747558594\n",
      "__Loss is  52.96784591674805\n",
      "__Loss is  52.059688568115234\n",
      "__Loss is  50.667728424072266\n",
      "__Loss is  50.107643127441406\n",
      "__Loss is  50.01691818237305\n",
      "__Loss is  50.03234100341797\n",
      "Learning Rate is  0.64\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.015647888183594\n",
      "__Loss is  51.20065689086914\n",
      "__Loss is  54.6200065612793\n",
      "__Loss is  52.571693420410156\n",
      "__Loss is  50.43525314331055\n",
      "__Loss is  50.02626419067383\n",
      "__Loss is  50.03898620605469\n",
      "__Loss is  50.3120002746582\n",
      "__Loss is  51.01785659790039\n",
      "Learning Rate is  0.81\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.0582275390625\n",
      "__Loss is  56.63241958618164\n",
      "__Loss is  54.503334045410156\n",
      "__Loss is  50.51946258544922\n",
      "__Loss is  50.01634216308594\n",
      "__Loss is  50.169795989990234\n",
      "__Loss is  51.275577545166016\n",
      "__Loss is  52.47325134277344\n",
      "__Loss is  51.7619514465332\n",
      "Learning Rate is  1.0\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.62932586669922\n",
      "__Loss is  63.91477966308594\n",
      "__Loss is  53.5537223815918\n",
      "__Loss is  50.044315338134766\n",
      "__Loss is  50.25554656982422\n",
      "__Loss is  53.2360954284668\n",
      "__Loss is  55.147239685058594\n",
      "__Loss is  52.235145568847656\n",
      "__Loss is  50.27403259277344\n",
      "Learning Rate is  1.21\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  53.30394744873047\n",
      "__Loss is  59.38713073730469\n",
      "__Loss is  50.467681884765625\n",
      "__Loss is  50.05082321166992\n",
      "__Loss is  52.03352737426758\n",
      "__Loss is  54.43857955932617\n",
      "__Loss is  51.476463317871094\n",
      "__Loss is  50.25464630126953\n",
      "__Loss is  50.798519134521484\n",
      "Learning Rate is  1.44\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  60.45414733886719\n",
      "__Loss is  52.94520568847656\n",
      "__Loss is  50.01652908325195\n",
      "__Loss is  50.96302795410156\n",
      "__Loss is  54.478919982910156\n",
      "__Loss is  51.76219940185547\n",
      "__Loss is  50.15760803222656\n",
      "__Loss is  50.35712432861328\n",
      "__Loss is  50.20903015136719\n",
      "Learning Rate is  1.69\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  72.90203094482422\n",
      "__Loss is  52.83784866333008\n",
      "__Loss is  50.08375930786133\n",
      "__Loss is  57.57798385620117\n",
      "__Loss is  56.49380111694336\n",
      "__Loss is  50.35950469970703\n",
      "__Loss is  50.386016845703125\n",
      "__Loss is  50.29490661621094\n",
      "__Loss is  52.37453842163086\n",
      "Learning Rate is  1.96\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  86.64501953125\n",
      "__Loss is  69.27193450927734\n",
      "__Loss is  50.049625396728516\n",
      "__Loss is  70.75753784179688\n",
      "__Loss is  67.39207458496094\n",
      "__Loss is  50.61168670654297\n",
      "__Loss is  57.69731903076172\n",
      "__Loss is  52.627410888671875\n",
      "__Loss is  54.45869445800781\n",
      "Learning Rate is  2.25\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  92.88108825683594\n",
      "__Loss is  50.01734161376953\n",
      "__Loss is  84.0397720336914\n",
      "__Loss is  58.2736930847168\n",
      "__Loss is  52.426910400390625\n",
      "__Loss is  65.75603485107422\n",
      "__Loss is  51.47699737548828\n",
      "__Loss is  50.19687271118164\n",
      "__Loss is  57.580631256103516\n",
      "Learning Rate is  2.56\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  85.02592468261719\n",
      "__Loss is  86.91867065429688\n",
      "__Loss is  83.52674865722656\n",
      "__Loss is  50.031410217285156\n",
      "__Loss is  76.49500274658203\n",
      "__Loss is  69.89494323730469\n",
      "__Loss is  51.732215881347656\n",
      "__Loss is  56.776119232177734\n",
      "__Loss is  53.525577545166016\n",
      "Learning Rate is  2.89\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  67.67581939697266\n",
      "__Loss is  83.20044708251953\n",
      "__Loss is  92.21916961669922\n",
      "__Loss is  79.6937484741211\n",
      "__Loss is  72.59501647949219\n",
      "__Loss is  51.48208236694336\n",
      "__Loss is  85.17192840576172\n",
      "__Loss is  54.94440460205078\n",
      "__Loss is  83.01423645019531\n",
      "Learning Rate is  3.24\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  54.03266906738281\n",
      "__Loss is  53.38031768798828\n",
      "__Loss is  53.63116455078125\n",
      "__Loss is  53.93183517456055\n",
      "__Loss is  53.953487396240234\n",
      "__Loss is  53.749603271484375\n",
      "__Loss is  53.52699661254883\n",
      "__Loss is  53.437625885009766\n",
      "__Loss is  53.4881477355957\n",
      "Learning Rate is  3.61\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.1474609375\n",
      "__Loss is  60.43773651123047\n",
      "__Loss is  64.91136932373047\n",
      "__Loss is  59.86617660522461\n",
      "__Loss is  53.210655212402344\n",
      "__Loss is  51.34616470336914\n",
      "__Loss is  52.46224594116211\n",
      "__Loss is  55.88532638549805\n",
      "__Loss is  57.59920120239258\n",
      "Learning Rate is  4.0\n",
      "__Loss is  56.78873062133789\n",
      "__Loss is  50.11730194091797\n",
      "__Loss is  51.489925384521484\n",
      "__Loss is  64.5628890991211\n",
      "__Loss is  62.75674057006836\n",
      "__Loss is  52.616859436035156\n",
      "__Loss is  50.3204460144043\n",
      "__Loss is  50.865806579589844\n",
      "__Loss is  55.5730094909668\n",
      "__Loss is  57.789878845214844\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "learningR=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "output_shape=2;\n",
    "epochs = 10     # set number of epochs\n",
    "\n",
    "model=[];\n",
    "lossLR=[];\n",
    "\n",
    "for l in learningR:\n",
    "    np.random.seed(2)  \n",
    "    qnn2 = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn2.num_weights) - 1)\n",
    "    # set up PyTorch module\n",
    "    model2 = TorchConnector(qnn2, initial_weights)\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.Adam(model2.parameters(), lr=l)\n",
    "    f_loss = MSELoss(reduction='sum')\n",
    "\n",
    "    # start training\n",
    "    model2.train()   # set model to training mode\n",
    "    \n",
    "    print(\"Learning Rate is \", l)\n",
    "    # define objective function\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(X, y01):                        # evaluate batch loss\n",
    "            output = model2(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            #targets = targets.unsqueeze(1)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    model.append(model2)\n",
    "    lossLR.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01 loss: 54.253997802734375  Accuracy: 0.49\n",
      "Learning Rate: 0.04 loss: 50.66582489013672  Accuracy: 0.49\n",
      "Learning Rate: 0.09 loss: 50.01509094238281  Accuracy: 0.45\n",
      "Learning Rate: 0.16 loss: 50.33076477050781  Accuracy: 0.49\n",
      "Learning Rate: 0.25 loss: 51.060794830322266  Accuracy: 0.49\n",
      "Learning Rate: 0.36 loss: 50.12599182128906  Accuracy: 0.48\n",
      "Learning Rate: 0.49 loss: 50.03234100341797  Accuracy: 0.49\n",
      "Learning Rate: 0.64 loss: 51.01785659790039  Accuracy: 0.49\n",
      "Learning Rate: 0.81 loss: 51.7619514465332  Accuracy: 0.47\n",
      "Learning Rate: 1.0 loss: 50.27403259277344  Accuracy: 0.52\n",
      "Learning Rate: 1.21 loss: 50.798519134521484  Accuracy: 0.52\n",
      "Learning Rate: 1.44 loss: 50.20903015136719  Accuracy: 0.51\n",
      "Learning Rate: 1.69 loss: 52.37453842163086  Accuracy: 0.51\n",
      "Learning Rate: 1.96 loss: 54.45869445800781  Accuracy: 0.49\n",
      "Learning Rate: 2.25 loss: 57.580631256103516  Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "ln=len(learningR)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "accLR=[];\n",
    "for lr in range(ln):\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+1)\n",
    "    y_predict = []\n",
    "    for x in X:\n",
    "        output = model[lr](Tensor(x))\n",
    "        y_predict += [np.argmax(output.detach().numpy())]\n",
    "    acc=sum(y_predict == np.array(y01))/len(np.array(y01))\n",
    "    print('Learning Rate:', learningR[lr],'loss:',lossLR[lr],' Accuracy:', acc)\n",
    "    accLR.append(acc)\n",
    "    # plot results\n",
    "    # red == wrongly classified\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_target != y_:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='r', linewidths=2)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+2)\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model[lr](Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fig\n",
    "f1.set_size_inches(10, 80)\n",
    "f1.savefig('Adam_2c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLR50=[x/100 for x in lossLR]\n",
    "plt.plot(learningR,lossLR50, label = \"Loss/100\")\n",
    "plt.plot(learningR,accLR, label = \"Accuracy\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lvA=[[learningR[x],accLR[x]] for x in range(len(accLR))]\n",
    "np.savetxt(\"Adam_Accuracy_2c.txt\",lvA,fmt='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
