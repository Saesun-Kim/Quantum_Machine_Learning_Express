{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "data0Path = r'../../../dataset/data3a.txt'\n",
    "data0Label = r'../../../dataset/data3alabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data1 = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return ('0'*(4-len('{:b}'.format(x) ))+'{:b}'.format(x))\n",
    "def firsttwo(x):\n",
    "    return x[:2]\n",
    "parity = lambda x: firsttwo(binary(x)).count('1') % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKoAAADWCAYAAABBlhk4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAARHklEQVR4nO3df1TUdb7H8Se/FPEXGaWpaaDCKgqrliXmBcryR96ibv5It5PmCRYsE/G21+yXq2lbmN5uhlbu0m6p94aeZE3c1ARRoM0KE22jAEUKLUVRlPDCeP9gh2AYmMFm+Hw+1/fjnP7oSzivznmd73e+M+O8PC5fvnwZITTnqTqAEM6QogojSFGFEaSowghSVGEEKaowghRVGEGKKowgRRVGkKIKI0hRhRGkqMIIUlRhBCmqMIIUVRhBiiqMIEUVRpCiCiNIUYURpKjCCFJUYQQpqjCCFFUYQYoqjCBFFUaQogojeKsOoLuvP4bzP6h57K7XQ8gdV/a7qnL/ksytkaI6cP4HOFumOkXbmZq7JXLpF0aQogojSFGFEaSowghyM+UiSSlRfHUsFy8vHzw9veh1TSAz7lxMZPgU1dFaZFJmKaoLzRz3LDPHPUNdXS1bc15nxYYZDOwznD4BA1VHa5EpmeXS7wZeXt5MvPUx6iy1FH2frzqOU3TPLEV1g/+tvcS2nBQA+gYEK07jHN0zy6XfhTbsfpH3s5KprjmPl5cPC6a8TVDvMACWvzeDO4bP4LYhkwF4PjWGfx2dwM0hd6uM3GrmjL+vZ9dnf2n4b8srihkWOJZFM95r95xan1EtFgvJyckMGjQIX19fwsPDycrKIiQkhNjYWNXxmplx52I+WHqWtBdOMepXkzj47Z6Gn8Xft5rUvz1LdU0V2Ye20Nm3u/KSQuuZJ46aw8r4TFbGZ7J45iZ8O3Rm9oQXleTUuqhz5sxh6dKlxMXFkZGRwdSpU3nooYcoLi5m5MiRquO1qKvfNSyY8jaf/ONDcgq2AnBNl+u5//YnWbN1Hht2L+O3965SnLIpe5mtLBYLKzbOZM7EFfTqcZOSfNoWdePGjaSmppKens7ChQuJjo5m8eLFjB49mtraWkaMGKE6Yqu6+fXg38Yu4I87nsZisQAw/pZZlP1YSMyYeXTz66E4YXP2MgP8ZecSAnsNY8zQGGXZtC3q8uXLmTBhApGRkU2ODxw4EB8fH8LC6p9HHT16lMjISIKDgxk2bBjZ2dkq4tp1/9gnqThXzs7P/txwrPe1A7V76acx28yff7Obzwo/4rF7XlaaS8uilpWVUVBQwJQpzV94Li0tJTQ0lI4dOwIQFxfHtGnTKCwsZN26dUyfPp1Lly45fAwPDw+n/snKynQq88r4TGaOe6bJsc6+3djy+wrG3zLLqT/DVlZWptM5ryS3o8wV507w+geP8/TMjfh4d3BLZmdpeddfVlb/+bRevXo1OV5dXU1WVhYTJ04E4NSpU+zbt4/09HQAIiIi6N27N3v27GH8+PHtG/r/oXd3LeXCT5W88t+zGo7deF0I8x9c1+5ZtCxqQEAAAIWFhUyaNKnh+Msvv0x5eXnDjVRpaSk9e/ZsOLsCBAYGcuzYMYeP4ewE7IFNrv1c51PTU53+byMjo7iccmVTta7IPe+BNcx7YE2bfueXZG6NlkUNCgoiLCyM5cuX06NHD/r06UNaWhrbt28H0PqOX7iHls9RPT09ef/99wkNDSU+Pp7Zs2cTEBDA3Llz8fLyariR6tevHydPnqSmpqbhd0tKSujfv7+q6MJNtDyjAgQHB7Nnz54mxx5++GGGDBlCp06dgPqnCGPGjGH9+vUkJCSQk5PDd999R3R0tIrIwo20PKO25MCBA80u+2vXrmXTpk0EBwcTGxvLxo0b6dDBuTtUd1m/fREL3vgX1m9fBEBKeiKJb4xlzdYnleZqjW1mgM17VzF/ze0KU/3MmKJWVVVRWFjY7IX+oKAg9u7dS2FhIQUFBc1ed21vJScKuPDTOV5N2Mu5i6cpKNlPdU0VqxKyqa29xNfHP1Wazx7bzEdPHOZSbY1Wn6IypqhdunShrq6OJ554QnWUVhWU7OPm4Pr38EcMuovi8oOMDL7rn/8+jiPHclXGs8s286GSbHb8fT133fyI4mQ/M6aopjh/sYJ3PnqepJQoNux+karqs/h17AZAZ9/uVFWfVRvQDtvMZ8+f5GBRJsMHuuEv6F8hbW+mTNXVrwePjP89EaH3kndkGyfPHONizTkALtSco0snf7UB7bDN/MPZUu7oPUN1rCbkjOpiQwNv51DxXgAOFmUyoPev+eKb3QB88c0uBve7TWU8u2wz5xft4a+5KSx6awLHTh7mg33/pTihFNXlAnsNxdvLh6SUKLy9fBgaOAYfH18S3xiLp6cXv+o3SnXEZmwzP/fw+7z02N9Y8dgO+vcMJeZ29fcFHpedfS/xKuXqt1Dbwr8v3Dz9yn5XVe5fkrk1ckYVRpCiCiPIXb8DXa8387FV5XbX48pzVGEEufQLI0hRhRGkqMIIUlRhBCmqMIIUVRhBiiqMIEUVRpCiCiNIUYURpKjCCFJUYQQpqjCCfMzPAVmXbhtZl1bE1JVmU3O3RC79wghSVGEEKaowgjxHdRGTBnCtTMosRXUhUwZwGzMls1z63UD3AVx7dM8sRXUD3Qdw7dE9s1z6XUhGe91H6zOqjPa6n4z2uoCM9rYfGe29QjLa2/5ktPcKODva+9xzzxEcHIynpydpaWkqorZIRntdR8ubKetob2JiYrOf2Y72TpgwgVmzZvHoo4+2d8wmVsZnNjtmHcDVlaPM1tHe5XMynB7tdRctz6iORnsbX/YjIiIICgpq82O4el3aHdy9Lu1I49HepJQoklKiWJ0W59LMztLyjOrsaK+J2jLaq9qVjPa6i5ZFbY/RXlXr0m2hel36SrhrXVrLS7+zo73i6qHlGRWcG+0VVw8tz6gtsTfa++yzz9K3b19yc3OJi4ujb9++FBUVKUpYr/EA7qnK74lfPYJJi3ypq6tVmqs1jTOfqDjKlCU9SUqJ4ndvqn/3DAwqakujvUuXLqWsrIyamhpOnz5NWVkZAwYMUJSy+QBuVfUZXo7dreUQmpVt5p8uXWDkoLtYGZ/JH2I/Uh0PMKiopo72HirJpqvfNYpTtc5e5vyiPSS+MZbNe/V4q1fb56imOn+xgm25a9mcvYqq6rNEhk9VHckh28y3D72fP/2ukA5eHXku9T6GD7yz4RNVqkhRXcx2APfHSv3/zrK9zJ06dAbgtsGTOXqyQHlRjbn0m8J2AHdY4FjFiRyzzTyk/+iGnx0+up8brlX3nN9KiupitgO4fa8L5ql14yguP8h/vD2er0o/UR2xGdvMpyq/I2H1SJ58PYJru/dhcL9bVUeUQTRHZLS3bWS0V1zVpKjCCHLX74CM9urxuPIcVRhBLv3CCFJUYQQpqjCCFFUYQYoqjCBFFUaQogojSFGFEaSowghSVGEEKaowghRVGEGKKowgH/NzQEZ720ZGexUxdfzW1NwtkUu/MIIUVRhBiiqMIEUVRpCbKRcxaanZyqTMUlQXMmWpuTFTMsul3w10X2q2R/fMUlQ30H2p2R7dM2t96bdYLLz66qusW7eO48ePExISwmuvvUZsbCyRkZG8+eabqiM2IevS7qP1GdW00V5Zl3Yfbc+o1tHezMzMhj3U6OhoPv/8c7Zs2aL1aK91qfmRlwaQU7CViKH3NVmXLvo+nz/E7lIdswl7ma1kXboVzoz2njlzhsmTJxMcHEx4eDh333033377raLETcm6tGtpWVTraO+UKc1fz2s82uvh4cH8+fMpLCzk4MGDTJ48mdmzZytIbJ+sS7uOtkUFx6O9/v7+jBs3ruHnERERlJSUOPUYrh6/XRmfycxxzzQ5Zl1qHn/LLKf+DFvuHu11lNm6Lv30zI1Or0vLaC+OR3tXr15NTExMe0S8KjRel7a68boQ5j+4rt2zaPm1kxaLheHDh1NeXk5ycnKT0d7S0lLy8vK49dam3yu/ZMkSMjIy+Pjjj/Hz83NZFvlq9La5qr4ava2jvcuWLWPbtm3s2LHDpSUV+tDy0g/Oj/YuWbKE7du3s3PnTvz9/ds5pWgv2hbVngMHDnDbbT9vih4+fJgXXniBAQMGEBUV1XA8Pz+//cMJtzKmqNbR3oSEhIZjoaGh6PYUe/32RRw+up/Qm8YQMTSGtemJeHh4EnLjLcTfq8euqD2Nc8+ZtIKdB/7MR5+9g8VSx6IZ7xHQvY/SfMYU1Traq7PGK82r0mKpq6vllbiP6eDjy4oNMykpP0TgDcNUx2zGNndByX6+LM7ilbjdqqM10PJmylS2K80lJw7RwccXAC/P+g8n68g2d3H5Qeosdfz7ujt5/YMnqLOoP0FIUV3o/MUK3vnoeZJSotiw+0XOX6wAoPj7L6m88CP9ew5RnNC+Zrmrz1Bbd4lX4nbT0cePnMNbVUc059JvAnsrzecuVvD6B4/zzG/+R3W8Ftnm/uFsKWFB9Z+x+PXAOygsO6A4oZxRXcreSvNLG39D7ORkenTr5eC31bHNHXRDOMXlXwJQ9H0+N/QIVBkPkKK6lO1K87GTRyg8/ilvffgUSSlRHDmaqzqiXba5hwaOoaNPJ5JSoig8/iljhz2oOqKeb6HqRN5CbZur6i1UIWxJUYUR5K7fAVmX1uNx5TmqMIJc+oURpKjCCFJUYQQpqjCCFFUYQYoqjCBFFUaQogojSFGFEaSowghSVGEEKaowghRVGEE+5ueArEu3jaxLK2LqSrOpuVsil35hBCmqMIIUVRhBnqO6iEkDuFYmZZaiupApA7iNmZJZLv1uoPsArj26Z5aiuoHuA7j26J5ZLv0uJKO97qP1GdVisZCcnMygQYPw9fUlPDycrKwsQkJCiI2NVR2vGRntdR+ti2raurSVdQD3k398SE5B/ZfgNh7t3bB7Gb/V7Pv87WW2ktHeVljXpdPT01m4cCHR0dEsXryY0aNHU1tbq/W6NMhor6tpW1Rn1qUBYmJiCAsLY/jw4YwaNYpdu/SZF5fRXtfR8mbKui6dmJjY7GeN16UBUlNTG4bQvvjiC6KioqioqMDLq32HHVbGZzY7Zh3A1ZWjzNbR3uVzMpwe7XUXLc+ozq5LA03W+iorK/Hw8HBqe8rV69Lu4O51aUcaj/YmpUSRlBLF6rQ4l2Z2lpZn1LauS8+dO5eMjAwqKyvZvHkz3t5a/m8B8NT0VNURnDbvgTXMe2CN6hiApl87eSXr0gBZWVkkJiayd+9eunTp4pIs8tXobXNVfTV6W9elrSIjI/H09GT//v3tnFi4m7bXSGfWpauqqjh9+jT9+/cH6m+mioqKGDx4cLvnFe6lbVHtsV2XvnDhAtOmTaOqqgpvb298fX1599136devn8KUTQdw7xgxk9VpsXh6etH72oEsnPrHNt1EtJfGmcOCItm05yUAyn78mnkPpCh9DRU0vfTbY12XbnzH37NnT/Ly8igoKCA/P5+8vDzuuecehSmbDuCeu3gai6WO/3w8h1UJ2QBarODZss18nf+NDW+dXu/fjxGDxqmOaM4Z1YR1aWg+gHvkWC4DeocD4OPdkeu636gynl22mQ+VZHNTr1DKTxfj37UnnTq65sb0lzCmqKY4f7GCbblr2Zy9iqrqs0SGTyXncDp/yniaPgGD6Nb5WtURm7GXGWDfoS2MGXq/4nT1jLn0m8I6gLsyPpPZE5bR1a8HEaH38tbCAgL8+5J3ZJvqiM3YywyQ+9VfiRhyr+J09aSoLmZvuNfKr2M3Ovp0UhWtRbaZhwWOpeLcCXy8OmhzBZCiupjtAO7JiqMsSIlkQUokZ6pOMjJY/WdQbdlmvqlXKDmHtzI69D7V0Rpo+c6UTuSdqba5qt6ZEsKWFFUYQV6eckBGe/V4XHmOKowgl35hBCmqMIIUVRhBiiqMIEUVRpCiCiNIUYURpKjCCFJUYQQpqjCCFFUYQYoqjCBFFUaQogojSFGFEaSowghSVGEEKaowwv8B83QnTeNAcBsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 206.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "#data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "data_ixs = np.random.choice(len(data1), size=100)\n",
    "\n",
    "X= [np.array(list(flatten([data1[j][0],data1[j][0]]))) for j in data_ixs]\n",
    "y = [data1[j][1] for j in data_ixs]\n",
    "y01 =  [ (x + 1)/2 for x in y]\n",
    "X_ = Tensor(X)\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "\n",
    "num_inputs=4;\n",
    "\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "feature_map.ry(pi/4,0)\n",
    "feature_map.ry(pi/4,1)\n",
    "feature_map.ry(pi/4,2)\n",
    "feature_map.ry(pi/4,3)\n",
    "feature_map.rz(pi/4,0)\n",
    "feature_map.rz(pi/4,1)\n",
    "feature_map.rz(pi/4,2)\n",
    "feature_map.rz(pi/4,3)\n",
    "\n",
    "\n",
    "param_y=[];\n",
    "for i in range(12):\n",
    "    param_y.append((Parameter('θ'+str(i))))\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "for i in range(4):\n",
    "    ansatz.ry(param_y[i],i)\n",
    "for i in range(4):\n",
    "    ansatz.rz(param_y[i+4],i)\n",
    "\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.append(feature_map, range(num_inputs))\n",
    "qc.append(ansatz, range(num_inputs))\n",
    "\n",
    "ansatz.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saesun Kim\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Loss is  28.37653160095215\n",
      "__Loss is  27.167503356933594\n",
      "__Loss is  26.469091415405273\n",
      "__Loss is  26.048227310180664\n",
      "__Loss is  25.781166076660156\n",
      "__Loss is  25.6032657623291\n",
      "__Loss is  25.479660034179688\n",
      "__Loss is  25.390649795532227\n",
      "__Loss is  25.324575424194336\n",
      "__Loss is  25.27424430847168\n",
      "Learning Rate is  0.04\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  25.320119857788086\n",
      "__Loss is  25.157018661499023\n",
      "__Loss is  25.098230361938477\n",
      "__Loss is  25.06927490234375\n",
      "__Loss is  25.052532196044922\n",
      "__Loss is  25.04184341430664\n",
      "__Loss is  25.034534454345703\n",
      "__Loss is  25.029277801513672\n",
      "__Loss is  25.025333404541016\n",
      "Learning Rate is  0.09\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  25.009408950805664\n",
      "__Loss is  25.008506774902344\n",
      "__Loss is  25.00780487060547\n",
      "__Loss is  25.00725746154785\n",
      "__Loss is  25.00680923461914\n",
      "__Loss is  25.00644302368164\n",
      "__Loss is  25.006139755249023\n",
      "__Loss is  25.005876541137695\n",
      "__Loss is  25.005666732788086\n",
      "Learning Rate is  0.16\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  28.359655380249023\n",
      "__Loss is  28.188241958618164\n",
      "__Loss is  26.299663543701172\n",
      "__Loss is  31.6699275970459\n",
      "__Loss is  28.082183837890625\n",
      "__Loss is  42.09294891357422\n",
      "__Loss is  31.965417861938477\n",
      "__Loss is  35.581241607666016\n",
      "__Loss is  28.09389877319336\n",
      "Learning Rate is  0.25\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  46.15996551513672\n",
      "__Loss is  25.373790740966797\n",
      "__Loss is  26.6527042388916\n",
      "__Loss is  30.13677406311035\n",
      "__Loss is  27.537612915039062\n",
      "__Loss is  25.283632278442383\n",
      "__Loss is  27.5925235748291\n",
      "__Loss is  35.62912368774414\n",
      "__Loss is  46.94627380371094\n",
      "Learning Rate is  0.36\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  31.455841064453125\n",
      "__Loss is  25.38578987121582\n",
      "__Loss is  27.998506546020508\n",
      "__Loss is  26.434389114379883\n",
      "__Loss is  28.253942489624023\n",
      "__Loss is  25.23703384399414\n",
      "__Loss is  25.78207778930664\n",
      "__Loss is  35.81886291503906\n",
      "__Loss is  35.46909713745117\n",
      "Learning Rate is  0.49\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  25.130666732788086\n",
      "__Loss is  25.03197479248047\n",
      "__Loss is  25.018455505371094\n",
      "__Loss is  25.01124382019043\n",
      "__Loss is  25.007648468017578\n",
      "__Loss is  25.005847930908203\n",
      "__Loss is  25.00497055053711\n",
      "__Loss is  25.0045223236084\n",
      "__Loss is  25.004301071166992\n",
      "Learning Rate is  0.64\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  46.848541259765625\n",
      "__Loss is  29.088476181030273\n",
      "__Loss is  27.762481689453125\n",
      "__Loss is  26.379940032958984\n",
      "__Loss is  28.57953643798828\n",
      "__Loss is  32.953582763671875\n",
      "__Loss is  25.18837547302246\n",
      "__Loss is  25.56570053100586\n",
      "__Loss is  25.263534545898438\n",
      "Learning Rate is  0.81\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  25.08219337463379\n",
      "__Loss is  25.04472541809082\n",
      "__Loss is  25.0274715423584\n",
      "__Loss is  25.012819290161133\n",
      "__Loss is  25.004253387451172\n",
      "__Loss is  25.004133224487305\n",
      "__Loss is  25.00408172607422\n",
      "__Loss is  25.00406265258789\n",
      "__Loss is  25.004053115844727\n",
      "Learning Rate is  1.0\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  44.656620025634766\n",
      "__Loss is  25.135095596313477\n",
      "__Loss is  25.074750900268555\n",
      "__Loss is  25.139711380004883\n",
      "__Loss is  27.33115577697754\n",
      "__Loss is  43.90248107910156\n",
      "__Loss is  38.06034469604492\n",
      "__Loss is  29.90475845336914\n",
      "__Loss is  38.282955169677734\n",
      "Learning Rate is  1.21\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  25.02681541442871\n",
      "__Loss is  25.00839614868164\n",
      "__Loss is  25.00461769104004\n",
      "__Loss is  25.004091262817383\n",
      "__Loss is  25.00405502319336\n",
      "__Loss is  25.004056930541992\n",
      "__Loss is  25.004053115844727\n",
      "__Loss is  25.004056930541992\n",
      "__Loss is  25.004058837890625\n",
      "Learning Rate is  1.44\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  42.753318786621094\n",
      "__Loss is  45.45470428466797\n",
      "__Loss is  26.24322509765625\n",
      "__Loss is  36.80236053466797\n",
      "__Loss is  29.29840660095215\n",
      "__Loss is  40.27350616455078\n",
      "__Loss is  25.006458282470703\n",
      "__Loss is  25.004215240478516\n",
      "__Loss is  25.00406265258789\n",
      "Learning Rate is  1.69\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  30.058753967285156\n",
      "__Loss is  29.915943145751953\n",
      "__Loss is  30.852937698364258\n",
      "__Loss is  26.74730110168457\n",
      "__Loss is  35.32442855834961\n",
      "__Loss is  27.291704177856445\n",
      "__Loss is  25.03215217590332\n",
      "__Loss is  25.066699981689453\n",
      "__Loss is  25.648067474365234\n",
      "Learning Rate is  1.96\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  25.07781410217285\n",
      "__Loss is  28.13576316833496\n",
      "__Loss is  25.80320167541504\n",
      "__Loss is  25.02165412902832\n",
      "__Loss is  25.01942253112793\n",
      "__Loss is  25.006113052368164\n",
      "__Loss is  25.00443458557129\n",
      "__Loss is  25.004091262817383\n",
      "__Loss is  25.004056930541992\n",
      "Learning Rate is  2.25\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  30.612150192260742\n",
      "__Loss is  45.12630844116211\n",
      "__Loss is  28.86322593688965\n",
      "__Loss is  35.829132080078125\n",
      "__Loss is  25.44523811340332\n",
      "__Loss is  28.848772048950195\n",
      "__Loss is  25.328632354736328\n",
      "__Loss is  25.318056106567383\n",
      "__Loss is  34.05533218383789\n",
      "Learning Rate is  2.56\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  42.74066162109375\n",
      "__Loss is  44.92048263549805\n",
      "__Loss is  25.28019905090332\n",
      "__Loss is  25.58704376220703\n",
      "__Loss is  29.837860107421875\n",
      "__Loss is  25.17644500732422\n",
      "__Loss is  26.746084213256836\n",
      "__Loss is  31.653406143188477\n",
      "__Loss is  34.52989959716797\n",
      "Learning Rate is  2.89\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  40.56094741821289\n",
      "__Loss is  25.196125030517578\n",
      "__Loss is  43.517704010009766\n",
      "__Loss is  27.4273681640625\n",
      "__Loss is  25.451644897460938\n",
      "__Loss is  47.03303527832031\n",
      "__Loss is  25.09916114807129\n",
      "__Loss is  26.235645294189453\n",
      "__Loss is  30.45290184020996\n",
      "Learning Rate is  3.24\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  34.59074401855469\n",
      "__Loss is  25.284318923950195\n",
      "__Loss is  25.606340408325195\n",
      "__Loss is  26.329710006713867\n",
      "__Loss is  27.32178497314453\n",
      "__Loss is  28.482023239135742\n",
      "__Loss is  26.151809692382812\n",
      "__Loss is  25.400781631469727\n",
      "__Loss is  36.465293884277344\n",
      "Learning Rate is  3.61\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  32.15597152709961\n",
      "__Loss is  25.12818717956543\n",
      "__Loss is  25.071870803833008\n",
      "__Loss is  29.23661994934082\n",
      "__Loss is  26.15172576904297\n",
      "__Loss is  26.57052230834961\n",
      "__Loss is  26.451143264770508\n",
      "__Loss is  38.616390228271484\n",
      "__Loss is  37.561790466308594\n",
      "Learning Rate is  4.0\n",
      "__Loss is  28.37653160095215\n",
      "__Loss is  33.3465690612793\n",
      "__Loss is  28.894861221313477\n",
      "__Loss is  27.990535736083984\n",
      "__Loss is  29.95467185974121\n",
      "__Loss is  39.67695617675781\n",
      "__Loss is  30.508501052856445\n",
      "__Loss is  25.650426864624023\n",
      "__Loss is  39.26573181152344\n",
      "__Loss is  33.132171630859375\n"
     ]
    }
   ],
   "source": [
    "learningR=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "\n",
    "    \n",
    "output_shape=2;\n",
    "epochs = 10     # set number of epochs\n",
    "\n",
    "model=[];\n",
    "lossLR=[];\n",
    "\n",
    "\n",
    "\n",
    "for l in learningR:\n",
    "    np.random.seed(2)  \n",
    "    qnn2 = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn2.num_weights) - 1)\n",
    "    # set up PyTorch module\n",
    "    model2 = TorchConnector(qnn2, initial_weights)\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.SGD(model2.parameters(),lr=l)\n",
    "    f_loss = MSELoss(reduction='mean')\n",
    "\n",
    "    # start training\n",
    "    model2.train()   # set model to training mode\n",
    "    \n",
    "    print(\"Learning Rate is \", l)\n",
    "    # define objective function\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(X, y01):                        # evaluate batch loss\n",
    "            output = model2(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            #targets = targets.unsqueeze(1)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    model.append(model2)\n",
    "    lossLR.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01 loss: 25.27424430847168  Accuracy: 0.54\n",
      "Learning Rate: 0.04 loss: 25.025333404541016  Accuracy: 0.71\n",
      "Learning Rate: 0.09 loss: 25.005666732788086  Accuracy: 0.55\n",
      "Learning Rate: 0.16 loss: 28.09389877319336  Accuracy: 0.54\n",
      "Learning Rate: 0.25 loss: 46.94627380371094  Accuracy: 0.56\n",
      "Learning Rate: 0.36 loss: 35.46909713745117  Accuracy: 0.54\n",
      "Learning Rate: 0.49 loss: 25.004301071166992  Accuracy: 0.41\n",
      "Learning Rate: 0.64 loss: 25.263534545898438  Accuracy: 0.54\n",
      "Learning Rate: 0.81 loss: 25.004053115844727  Accuracy: 0.45\n",
      "Learning Rate: 1.0 loss: 38.282955169677734  Accuracy: 0.46\n",
      "Learning Rate: 1.21 loss: 25.004058837890625  Accuracy: 0.45\n",
      "Learning Rate: 1.44 loss: 25.00406265258789  Accuracy: 0.45\n",
      "Learning Rate: 1.69 loss: 25.648067474365234  Accuracy: 0.46\n",
      "Learning Rate: 1.96 loss: 25.004056930541992  Accuracy: 0.45\n",
      "Learning Rate: 2.25 loss: 34.05533218383789  Accuracy: 0.54\n",
      "Learning Rate: 2.56 loss: 34.52989959716797  Accuracy: 0.54\n",
      "Learning Rate: 2.89 loss: 30.45290184020996  Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "ln=len(learningR)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "accLR=[];\n",
    "for lr in range(ln):\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+1)\n",
    "    y_predict = []\n",
    "    for x in X:\n",
    "        output = model[lr](Tensor(x))\n",
    "        y_predict += [np.argmax(output.detach().numpy())]\n",
    "    acc=sum(y_predict == np.array(y01))/len(np.array(y01))\n",
    "    print('Learning Rate:', learningR[lr],'loss:',lossLR[lr],' Accuracy:', acc)\n",
    "    accLR.append(acc)\n",
    "    # plot results\n",
    "    # red == wrongly classified\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_target != y_:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='r', linewidths=2)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+2)\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model[lr](Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fig\n",
    "f1.set_size_inches(10, 80)\n",
    "f1.savefig('SGD_3a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLR50=[x/50 for x in lossLR]\n",
    "plt.plot(learningR,lossLR50, label = \"Loss/50\")\n",
    "plt.plot(learningR,accLR, label = \"Accuracy\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lvA=[[learningR[x],accLR[x]] for x in range(len(accLR))]\n",
    "np.savetxt(\"SGD_Accuracy_3a.txt\",lvA,fmt='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
