{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afa6a21",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a unified interface to our code for doing data generation.\n",
    "\n",
    "We plan to do experiment with the following things:\n",
    "\n",
    "**Circuit**\n",
    "\n",
    "We will use circuits 1, 6, 9, and 14, as they cover a variety of circuit expressibilities and entangling capabilities.\n",
    "\n",
    "| | High Ent | Low Ent |\n",
    "| --- | --- | --- |\n",
    "| High Exp|6 | 14 |\n",
    "|Low Exp| 9 | 1 |\n",
    "\n",
    "**Data sets**\n",
    "\n",
    "We will use 4 data sets:\n",
    "\n",
    "* Data set 0 (2 blobs, separable)\n",
    "* Data set 1 (2 blobs, inseparable)\n",
    "* Data set 2a (4 blobs, separable)\n",
    "* Data set 3c (4 blobs) <-- Saesun's choice\n",
    "\n",
    "**Learning rates**\n",
    "\n",
    "We will investigate several learning rates.\n",
    "\n",
    "----------------\n",
    "\n",
    "The code below will help us set up our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769a087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is a hack in case Travis' kernel fails.\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK'] ='True'\n",
    "\n",
    "# Pull in the helper files.\n",
    "%run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaaa32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the experiment\n",
    "circuitID = 6\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0980fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------dataset 0 is initialized------\n",
      "__Learning Rate ( 0.01 ) is intialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim8104\\Anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Learning Rate ( 0.01 ) is done\n",
      "__Learning Rate ( 0.04 ) is intialized\n",
      "__Learning Rate ( 0.04 ) is done\n",
      "__Learning Rate ( 0.09 ) is intialized\n",
      "__Learning Rate ( 0.09 ) is done\n",
      "__Learning Rate ( 0.16 ) is intialized\n",
      "__Learning Rate ( 0.16 ) is done\n",
      "__Learning Rate ( 0.25 ) is intialized\n",
      "__Learning Rate ( 0.25 ) is done\n",
      "__Learning Rate ( 0.36 ) is intialized\n",
      "__Learning Rate ( 0.36 ) is done\n",
      "__Learning Rate ( 0.49 ) is intialized\n",
      "__Learning Rate ( 0.49 ) is done\n",
      "__Learning Rate ( 0.64 ) is intialized\n",
      "__Learning Rate ( 0.64 ) is done\n",
      "__Learning Rate ( 0.81 ) is intialized\n",
      "__Learning Rate ( 0.81 ) is done\n",
      "__Learning Rate ( 1.0 ) is intialized\n",
      "__Learning Rate ( 1.0 ) is done\n",
      "__Learning Rate ( 1.21 ) is intialized\n",
      "__Learning Rate ( 1.21 ) is done\n",
      "__Learning Rate ( 1.44 ) is intialized\n",
      "__Learning Rate ( 1.44 ) is done\n",
      "__Learning Rate ( 1.69 ) is intialized\n",
      "__Learning Rate ( 1.69 ) is done\n",
      "__Learning Rate ( 1.96 ) is intialized\n",
      "__Learning Rate ( 1.96 ) is done\n",
      "__Learning Rate ( 2.25 ) is intialized\n",
      "__Learning Rate ( 2.25 ) is done\n",
      "__Learning Rate ( 2.56 ) is intialized\n",
      "__Learning Rate ( 2.56 ) is done\n",
      "__Learning Rate ( 2.89 ) is intialized\n",
      "__Learning Rate ( 2.89 ) is done\n",
      "__Learning Rate ( 3.24 ) is intialized\n",
      "__Learning Rate ( 3.24 ) is done\n",
      "__Learning Rate ( 3.61 ) is intialized\n",
      "__Learning Rate ( 3.61 ) is done\n",
      "__Learning Rate ( 4.0 ) is intialized\n",
      "__Learning Rate ( 4.0 ) is done\n",
      "--------dataset 1a is initialized------\n",
      "__Learning Rate ( 0.01 ) is intialized\n",
      "__Learning Rate ( 0.01 ) is done\n",
      "__Learning Rate ( 0.04 ) is intialized\n",
      "__Learning Rate ( 0.04 ) is done\n",
      "__Learning Rate ( 0.09 ) is intialized\n",
      "__Learning Rate ( 0.09 ) is done\n",
      "__Learning Rate ( 0.16 ) is intialized\n",
      "__Learning Rate ( 0.16 ) is done\n",
      "__Learning Rate ( 0.25 ) is intialized\n",
      "__Learning Rate ( 0.25 ) is done\n",
      "__Learning Rate ( 0.36 ) is intialized\n",
      "__Learning Rate ( 0.36 ) is done\n",
      "__Learning Rate ( 0.49 ) is intialized\n",
      "__Learning Rate ( 0.49 ) is done\n",
      "__Learning Rate ( 0.64 ) is intialized\n",
      "__Learning Rate ( 0.64 ) is done\n",
      "__Learning Rate ( 0.81 ) is intialized\n",
      "__Learning Rate ( 0.81 ) is done\n",
      "__Learning Rate ( 1.0 ) is intialized\n",
      "__Learning Rate ( 1.0 ) is done\n",
      "__Learning Rate ( 1.21 ) is intialized\n",
      "__Learning Rate ( 1.21 ) is done\n",
      "__Learning Rate ( 1.44 ) is intialized\n",
      "__Learning Rate ( 1.44 ) is done\n",
      "__Learning Rate ( 1.69 ) is intialized\n",
      "__Learning Rate ( 1.69 ) is done\n",
      "__Learning Rate ( 1.96 ) is intialized\n",
      "__Learning Rate ( 1.96 ) is done\n",
      "__Learning Rate ( 2.25 ) is intialized\n",
      "__Learning Rate ( 2.25 ) is done\n",
      "__Learning Rate ( 2.56 ) is intialized\n",
      "__Learning Rate ( 2.56 ) is done\n",
      "__Learning Rate ( 2.89 ) is intialized\n",
      "__Learning Rate ( 2.89 ) is done\n",
      "__Learning Rate ( 3.24 ) is intialized\n",
      "__Learning Rate ( 3.24 ) is done\n",
      "__Learning Rate ( 3.61 ) is intialized\n",
      "__Learning Rate ( 3.61 ) is done\n",
      "__Learning Rate ( 4.0 ) is intialized\n",
      "__Learning Rate ( 4.0 ) is done\n",
      "--------dataset 2a is initialized------\n",
      "__Learning Rate ( 0.01 ) is intialized\n",
      "__Learning Rate ( 0.01 ) is done\n",
      "__Learning Rate ( 0.04 ) is intialized\n",
      "__Learning Rate ( 0.04 ) is done\n",
      "__Learning Rate ( 0.09 ) is intialized\n",
      "__Learning Rate ( 0.09 ) is done\n",
      "__Learning Rate ( 0.16 ) is intialized\n",
      "__Learning Rate ( 0.16 ) is done\n",
      "__Learning Rate ( 0.25 ) is intialized\n",
      "__Learning Rate ( 0.25 ) is done\n",
      "__Learning Rate ( 0.36 ) is intialized\n",
      "__Learning Rate ( 0.36 ) is done\n",
      "__Learning Rate ( 0.49 ) is intialized\n",
      "__Learning Rate ( 0.49 ) is done\n",
      "__Learning Rate ( 0.64 ) is intialized\n",
      "__Learning Rate ( 0.64 ) is done\n",
      "__Learning Rate ( 0.81 ) is intialized\n",
      "__Learning Rate ( 0.81 ) is done\n",
      "__Learning Rate ( 1.0 ) is intialized\n",
      "__Learning Rate ( 1.0 ) is done\n",
      "__Learning Rate ( 1.21 ) is intialized\n",
      "__Learning Rate ( 1.21 ) is done\n",
      "__Learning Rate ( 1.44 ) is intialized\n",
      "__Learning Rate ( 1.44 ) is done\n",
      "__Learning Rate ( 1.69 ) is intialized\n",
      "__Learning Rate ( 1.69 ) is done\n",
      "__Learning Rate ( 1.96 ) is intialized\n",
      "__Learning Rate ( 1.96 ) is done\n",
      "__Learning Rate ( 2.25 ) is intialized\n",
      "__Learning Rate ( 2.25 ) is done\n",
      "__Learning Rate ( 2.56 ) is intialized\n",
      "__Learning Rate ( 2.56 ) is done\n",
      "__Learning Rate ( 2.89 ) is intialized\n",
      "__Learning Rate ( 2.89 ) is done\n",
      "__Learning Rate ( 3.24 ) is intialized\n",
      "__Learning Rate ( 3.24 ) is done\n",
      "__Learning Rate ( 3.61 ) is intialized\n",
      "__Learning Rate ( 3.61 ) is done\n",
      "__Learning Rate ( 4.0 ) is intialized\n",
      "__Learning Rate ( 4.0 ) is done\n",
      "--------dataset 3c is initialized------\n",
      "__Learning Rate ( 0.01 ) is intialized\n",
      "__Learning Rate ( 0.01 ) is done\n",
      "__Learning Rate ( 0.04 ) is intialized\n",
      "__Learning Rate ( 0.04 ) is done\n",
      "__Learning Rate ( 0.09 ) is intialized\n",
      "__Learning Rate ( 0.09 ) is done\n",
      "__Learning Rate ( 0.16 ) is intialized\n",
      "__Learning Rate ( 0.16 ) is done\n",
      "__Learning Rate ( 0.25 ) is intialized\n",
      "__Learning Rate ( 0.25 ) is done\n",
      "__Learning Rate ( 0.36 ) is intialized\n",
      "__Learning Rate ( 0.36 ) is done\n",
      "__Learning Rate ( 0.49 ) is intialized\n",
      "__Learning Rate ( 0.49 ) is done\n",
      "__Learning Rate ( 0.64 ) is intialized\n",
      "__Learning Rate ( 0.64 ) is done\n",
      "__Learning Rate ( 0.81 ) is intialized\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lr_list=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "ds_list=['0','1a','2a','3c']\n",
    "\n",
    "for dsID in ds_list:\n",
    "# Run the experiment\n",
    "    print('--------dataset',dsID,'is initialized------')\n",
    "    lr_acc=[]\n",
    "    for lr in lr_list:\n",
    "        # Load in the data\n",
    "        data = load_data(dsID)\n",
    "\n",
    "        # Generate the splittings\n",
    "        train_X, train_y, validate_X, validate_y, test_X, test_y = generate_train_validate_test_data(data)\n",
    "\n",
    "        # Make the feature map\n",
    "        feature_map= make_embedding_circuit()\n",
    "\n",
    "        # Make the classifier\n",
    "        ansatz = make_classifer_circuit(circuitID)\n",
    "\n",
    "        # Do the training\n",
    "        model = train_model(feature_map, ansatz, epochs, lr, train_X, train_y)\n",
    "\n",
    "        # Check the validation accuracy.\n",
    "        val_accuracy = check_accuracy(model, validate_X, validate_y)\n",
    "\n",
    "        lr_acc.append([lr,val_accuracy])\n",
    "    np.savetxt(r\"Learning_Rate_Data\\circuit{0}_data{1}.txt\".format(circuitID,dsID),lr_acc,fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb500bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------dataset 3c is initialized------\n",
      "__Learning Rate ( 0.01 ) is intialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim8104\\Anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lr_list=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "dsID='3c'\n",
    "\n",
    "print('--------dataset',dsID,'is initialized------')\n",
    "lr_acc=[]\n",
    "for lr in lr_list:\n",
    "    # Load in the data\n",
    "    data = load_data(dsID)\n",
    "\n",
    "    # Generate the splittings\n",
    "    train_X, train_y, validate_X, validate_y, test_X, test_y = generate_train_validate_test_data(data)\n",
    "\n",
    "    # Make the feature map\n",
    "    feature_map= make_embedding_circuit()\n",
    "\n",
    "    # Make the classifier\n",
    "    ansatz = make_classifer_circuit(circuitID)\n",
    "\n",
    "    # Do the training\n",
    "    model = train_model(feature_map, ansatz, epochs, lr, train_X, train_y)\n",
    "\n",
    "    # Check the validation accuracy.\n",
    "    val_accuracy = check_accuracy(model, validate_X, validate_y)\n",
    "\n",
    "    lr_acc.append([lr,val_accuracy])\n",
    "np.savetxt(r\"Learning_Rate_Data\\circuit{0}_data{1}.txt\".format(circuitID,dsID),lr_acc,fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the experiment\n",
    "circuitID = 9\n",
    "epochs = 20\n",
    "ds_list=['0','1a','2a','3c']\n",
    "#selected learning rates\n",
    "lr_opt=[0.25,0.25,0.25,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_list=[]\n",
    "for i in range(4):\n",
    "    data = load_data(ds_list[i])\n",
    "\n",
    "    # Generate the splittings\n",
    "    train_X, train_y, validate_X, validate_y, test_X, test_y = generate_train_validate_test_data(data)\n",
    "\n",
    "    # Make the feature map\n",
    "    feature_map= make_embedding_circuit()\n",
    "\n",
    "    # Make the classifier\n",
    "    ansatz = make_classifer_circuit(circuitID)\n",
    "\n",
    "    # Do the training\n",
    "    model = train_model(feature_map, ansatz, epochs, lr_opt[i], train_X, train_y)\n",
    "    \n",
    "    model_list.append(model)\n",
    "    # Check the validation accuracy.\n",
    "    val_accuracy = check_accuracy(model, test_X, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3810jvsc74a57bd07085d7f3a85dff14807db3b039303f4d3e7814f3ae962158c22b2b6462c5d354",
   "display_name": "Python 3.8.10 64-bit ('qiskit': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "7085d7f3a85dff14807db3b039303f4d3e7814f3ae962158c22b2b6462c5d354"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}