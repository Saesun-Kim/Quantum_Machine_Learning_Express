{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "#load the simulation functcions from the quantumcircuit.py file\n",
    "\n",
    "from quantumcircuit import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function Two-Qubit\n",
    "\n",
    "Loss is calculated with a simple two-qubit gate. First, the data is embedded into Rx rotation with pi/4 rotation of Ry and Rz. The parameterized quantum circuit is placed after the embedding. We used same structure of the circuit1 from the paper to check the convergence.\n",
    "\n",
    "![Two-Qubits Example](Two-qubits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [SPSA method](https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation)\n",
    "\n",
    "SPSA is an algorithm used in numerical optimization.\n",
    "\n",
    "Consider a loss function $L(\\mathbf{x})$. In order to optimize $L$, SPSA iteratively optimizes the parameters of the loss function according to the following update rule:\n",
    "\n",
    "$$\\mathbf{y}_{n+1} = \\mathbf{y}_{n} - a_{n}\\widehat{\\Delta L}_{n}(\\mathbf{y}_{n}),$$\n",
    "\n",
    "where $\\widehat{\\Delta L}_{n}(\\mathbf{y}_{n}))$ is an estimate of the gradient at $\\mathbf{y}_{n}$.\n",
    "\n",
    "How SPSA calculates that gradient is by using a finite-difference rule on random perturbations of component of the parameter vector.\n",
    "    $$\\left( \\widehat{\\Delta L}_{n}(\\mathbf{y}_{n}))\\right)_{j} = \\frac{L(\\mathbf{y}_{n} + c_{n}\\boldsymbol{\\Delta}_{n}) - L(\\mathbf{y}_{n} - c_{n}\\boldsymbol{\\Delta}_{n}) }{c_{n}\\left(\\boldsymbol{\\Delta}_{n}\\right)_{j}}$$\n",
    "    \n",
    "That is at each iteration, SPSA generates a random perturbation $\\boldsymbol{\\Delta}_{n}$, and does a finite-difference rule with that perturbation.\n",
    "\n",
    "In order for the algorithm to converge, the random perturbation $\\boldsymbol{\\Delta}_{n}$ and finite-difference coefficients $c_{n}$ must satisfy certain properties (see the Wikipedia article linked to in the title). Importantly $c_{n}$ must tend to 0 as $n$ increases, and the size of the random fluctuations $\\boldsymbol{\\Delta}_{n}$ must also be bounded.\n",
    "\n",
    "According to the wikipedia article, the coefficients $a_{n}$ and $c_{n}$ should look something like\n",
    "\n",
    "$$c_{n} = \\frac{c}{n^\\gamma}~~\\text{and}~~a_{n} = \\frac{a}{n}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the SPSA optimizer from the optimizer.py file\n",
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the convergence of 100 data point\n",
    "\n",
    "To have sanity check, we take 100 data and run the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iteration L= 0.0 Accuracy = 0.99\n",
      "1 th iteration L= 0.0 Accuracy = 1.0\n",
      "2 th iteration L= 0.08 Accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "data1Path = r'../dataset/data0test.txt'\n",
    "data1Label = r'../dataset/data0testlabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "ax.plot(lossList)\n",
    "ax.set_title('Loss function\\nStart {0} Finish {1}'.format(np.round(lossList[0], 3), np.round(lossList[-1], 3)))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.plot(accuracyList)\n",
    "ax.set_title('Classification accuracy \\nStart {0} Finish {1}'.format(np.round(accuracyList[0], 3), np.round(accuracyList[-1], 3)))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.plot([c[0] for c in coeffsList], label='a')\n",
    "ax.plot([c[1] for c in coeffsList], label='c')\n",
    "ax.legend(loc=0)\n",
    "ax.set_title('Update coefficients')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "for j in range(4):\n",
    "    ax.plot([X[j] for X in paramsList], label='x{0}'.format(j))\n",
    "ax.legend(loc=0)\n",
    "ax.set_title('Parameter values')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(np.ravel(dataCoords[np.where(dataLabels == 0)])[::2],\n",
    "np.ravel(dataCoords[np.where(dataLabels == 0)])[1::2], ls='', marker='o')\n",
    "\n",
    "ax.plot(np.ravel(dataCoords[np.where(dataLabels == 1)])[::2],\n",
    "np.ravel(dataCoords[np.where(dataLabels == 1)])[1::2], ls='', marker='o')\n",
    "\n",
    "X = np.linspace(0, 1, num=20)\n",
    "Z = np.zeros((len(X), len(X)))\n",
    "\n",
    "# Contour map\n",
    "for j in range(len(X)):\n",
    "    for k in range(len(X)):\n",
    "        # Fill Z with the labels (numerical values)\n",
    "        # the inner loop goes over the columns of Z,\n",
    "        # which corresponds to sweeping x-values\n",
    "        # Therefore, the role of j,k is flipped in the signature\n",
    "        Z[j, k] = quick_predict( np.array([X[k], X[j]]),currentParams)\n",
    "        \n",
    "ax.contourf(X, X, Z, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data1a.txt'\n",
    "data1Label = r'../dataset/data1alabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "ax.plot(lossList)\n",
    "ax.set_title('Loss function\\nStart {0} Finish {1}'.format(np.round(lossList[0], 3), np.round(lossList[-1], 3)))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.plot(accuracyList)\n",
    "ax.set_title('Classification accuracy \\nStart {0} Finish {1}'.format(np.round(accuracyList[0], 3), np.round(accuracyList[-1], 3)))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.plot([c[0] for c in coeffsList], label='a')\n",
    "ax.plot([c[1] for c in coeffsList], label='c')\n",
    "ax.legend(loc=0)\n",
    "ax.set_title('Update coefficients')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "for j in range(4):\n",
    "    ax.plot([X[j] for X in paramsList], label='x{0}'.format(j))\n",
    "ax.legend(loc=0)\n",
    "ax.set_title('Parameter values')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data1b.txt'\n",
    "data1Label = r'../dataset/data1blabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "ax.plot(lossList)\n",
    "ax.set_title('Loss function\\nStart {0} Finish {1}'.format(np.round(lossList[0], 3), np.round(lossList[-1], 3)))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.plot(accuracyList)\n",
    "ax.set_title('Classification accuracy \\nStart {0} Finish {1}'.format(np.round(accuracyList[0], 3), np.round(accuracyList[-1], 3)))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.plot([c[0] for c in coeffsList], label='a')\n",
    "ax.plot([c[1] for c in coeffsList], label='c')\n",
    "ax.legend(loc=0)\n",
    "ax.set_title('Update coefficients')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "for j in range(4):\n",
    "    ax.plot([X[j] for X in paramsList], label='x{0}'.format(j))\n",
    "ax.legend(loc=0)\n",
    "ax.set_title('Parameter values')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data1c.txt'\n",
    "data1Label = r'../dataset/data1clabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data2a.txt'\n",
    "data1Label = r'../dataset/data2alabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data2b.txt'\n",
    "data1Label = r'../dataset/data2blabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data2c.txt'\n",
    "data1Label = r'../dataset/data2clabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data3a.txt'\n",
    "data1Label = r'../dataset/data3alabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data3b.txt'\n",
    "data1Label = r'../dataset/data3blabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Path = r'../dataset/data3c.txt'\n",
    "data1Label = r'../dataset/data3clabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data1Path)\n",
    "dataLabels = np.loadtxt(data1Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data = list(zip(dataCoords, 2*dataLabels-1))\n",
    "shuffled_data = shuffle(data)\n",
    "\n",
    "\n",
    "\n",
    "c = 1\n",
    "a = 1\n",
    "\n",
    "# Do the updates\n",
    "lossList = []\n",
    "coeffsList = []\n",
    "paramsList = []\n",
    "accuracyList = []\n",
    "\n",
    "np.random.seed(2)\n",
    "currentParams = pi*np.random.uniform(size=4)\n",
    "for j in range(10):      \n",
    "    \n",
    "    cj = c/(j+1)**(1/3)\n",
    "    aj = a/(j+1)\n",
    "\n",
    "    \n",
    "    # Grab a subset of the data for minibatching\n",
    "    #np.random.seed(j)\n",
    "    np.random.seed(2)\n",
    "    #data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "    data_ixs = np.random.choice(len(data), size=100)\n",
    "    \n",
    "    # Evaluate the loss over that subset\n",
    "    # We include a regularization term at the end \n",
    "    L = lambda x: np.sum([loss2qubit(data[j][0],data[j][1],x) for j in data_ixs])/len(data_ixs)\n",
    "    \n",
    "    lossList.append(L(currentParams))\n",
    "    coeffsList.append((cj, aj))\n",
    "    paramsList.append(currentParams)\n",
    "    accuracyList.append(np.sum([quick_predict(data[j][0],currentParams) ==data[j][1] for j in data_ixs])/len(data_ixs))\n",
    "    print(j,\"th iteration L=\",lossList[-1],\"Accuracy =\",accuracyList[-1])\n",
    "    currentParams = SPSA_update(L, currentParams, aj, cj)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
